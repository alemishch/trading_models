{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3883a7-db5c-425c-89de-8d026f055cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from scipy.stats import percentileofscore, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b86eb88-342f-464c-8071-2cad345002be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44376/534520918.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'BTCUSDT.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "train_start = '2020-01-01 00:00:00'\n",
    "train_end   = '2020-08-30 23:59:00'\n",
    "test_start  = '2020-09-01 00:00:00'\n",
    "test_end    = '2021-07-30 23:59:00'\n",
    "\n",
    "train_data = data.loc[train_start:train_end]\n",
    "test_data  = data.loc[test_start:test_end]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_data[['close']])\n",
    "test_scaled  = scaler.transform(test_data[['close']])\n",
    "\n",
    "train_torch = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "test_torch  = torch.tensor(test_scaled,  dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce62a765-03d2-468d-a86a-2d35269f30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data_tensor, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data_tensor) - seq_length):\n",
    "        seq = data_tensor[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "sequence_length = 100\n",
    "test_dates = test_data.index[sequence_length:]\n",
    "train_sequences = create_sequences(train_torch, sequence_length)\n",
    "test_sequences  = create_sequences(test_torch, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c646df4-e5fd-4bc0-b796-5ccea3cd1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims=[64, 32, 16]):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        curr_size = input_size\n",
    "        for hd in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(curr_size, hd))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            curr_size = hd\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        hidden_dims.reverse()\n",
    "        for hd in hidden_dims[1:]:\n",
    "            decoder_layers.append(nn.Linear(curr_size, hd))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            curr_size = hd\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(curr_size, input_size))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)      #(batch_size, seq_length*features) flatten\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        decoded = decoded.view(x.size(0), -1)  # reshape back\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3dbfc9-8b6e-4863-8bfe-391e4ec5ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, sequence_length=100, hidden_dims=[16, 32]):\n",
    "        super(CnnAutoencoder, self).__init__()\n",
    "        \n",
    "        assert len(hidden_dims) == 2, ('')\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        encoder_layers = []\n",
    "        \n",
    "        encoder_layers.append(nn.Conv1d(in_channels=input_channels,\n",
    "                                        out_channels=hidden_dims[0],\n",
    "                                        kernel_size=3,\n",
    "                                        padding=1))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        encoder_layers.append(nn.MaxPool1d(kernel_size=2, stride=2)) \n",
    "\n",
    "        encoder_layers.append(nn.Conv1d(in_channels=hidden_dims[0],\n",
    "                                        out_channels=hidden_dims[1],\n",
    "                                        kernel_size=3,\n",
    "                                        padding=1))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        encoder_layers.append(nn.MaxPool1d(kernel_size=2, stride=2)) \n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        self.encoded_length = sequence_length // 2  # first MaxPool\n",
    "        self.encoded_length = self.encoded_length // 2  # second MaxPool\n",
    "        \n",
    "        decoder_layers = []\n",
    "\n",
    "        decoder_layers.append(nn.ConvTranspose1d(\n",
    "            in_channels=hidden_dims[1],\n",
    "            out_channels=hidden_dims[0],\n",
    "            kernel_size=2, \n",
    "            stride=2\n",
    "        ))\n",
    "        decoder_layers.append(nn.ReLU())\n",
    "        \n",
    "        decoder_layers.append(nn.ConvTranspose1d(\n",
    "            in_channels=hidden_dims[0],\n",
    "            out_channels=input_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        ))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)    \n",
    "        decoded = self.decoder(encoded) \n",
    "        \n",
    "        if decoded.size(2) > x.size(2):\n",
    "            decoded = decoded[:, :, :x.size(2)]\n",
    "        elif decoded.size(2) < x.size(2):\n",
    "            pad_amount = x.size(2) - decoded.size(2)\n",
    "            decoded = nn.functional.pad(decoded, (0, pad_amount), \"constant\", 0.0)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0289d081-3769-4693-b551-4f1f7300af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8173d604-dc96-442b-80b3-f09b9c76d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based model for reconstruction-based anomaly detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, out_channels=16, kernel_size=3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 1 feature, channel=1\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "                               kernel_size=kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Flatten -> fully connected -> reshape\n",
    "        self.fc = nn.Linear(out_channels * input_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, 1) \n",
    "        x = x.transpose(1, 2)   # shape: (batch_size, 1, seq_len)\n",
    "        x = self.conv1(x)       # (batch_size, out_channels, seq_len)\n",
    "        x = self.relu(x)\n",
    "        x = x.reshape(x.size(0), -1)  # flatten to (batch_size, out_channels * seq_len)\n",
    "        x = self.fc(x)             # (batch_size, input_size)\n",
    "        return x.unsqueeze(-1)     # (batch_size, input_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b13834-3273-4445-901f-344f3e8641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackVAEG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims=[64, 32], latent_dim=16):\n",
    "        super(StackVAEG, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        in_dim = self.input_size\n",
    "        for hdim in self.hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(in_dim, hdim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            in_dim = hdim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(in_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(in_dim, self.latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        out_dim = self.hidden_dims[-1]\n",
    "        decoder_layers.append(nn.Linear(self.latent_dim, out_dim))\n",
    "        decoder_layers.append(nn.ReLU())\n",
    "        for hdim in reversed(self.hidden_dims[:-1]):\n",
    "            decoder_layers.append(nn.Linear(out_dim, hdim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            out_dim = hdim\n",
    "        decoder_layers.append(nn.Linear(out_dim, self.input_size))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        return reconstructed.view(x.size(0), -1), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8270f18-c312-4e00-a95c-f7aafad11562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniAnomaly(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, latent_dim=16, num_layers=1):\n",
    "        super(OmniAnomaly, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        self.lstm_enc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_dim)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        self.lstm_dec = nn.LSTM(latent_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        enc_out, _ = self.lstm_enc(x)  \n",
    "        mu = self.fc_mu(enc_out)       # (batch_size, seq_len, latent_dim)\n",
    "        logvar = self.fc_logvar(enc_out)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)  # (batch_size, seq_len, latent_dim)\n",
    "        \n",
    "        dec_out, _ = self.lstm_dec(z)\n",
    "        reconstructed = self.fc_out(dec_out)  # (batch_size, seq_len, input_size)\n",
    "        \n",
    "        return reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab87fdb-27bd-4daf-9d71-f5caaf3f1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGADSModel:\n",
    "    def __init__(self, window=30):\n",
    "        self.window = window\n",
    "        self.train_length_ = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        data = data.flatten()\n",
    "        self.train_length_ = len(data)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = data.flatten()\n",
    "        full_series = pd.Series(data)\n",
    "        \n",
    "        roll_mean = full_series.rolling(self.window, min_periods=1).mean()\n",
    "        roll_std = full_series.rolling(self.window, min_periods=1).std()\n",
    "\n",
    "        roll_std = roll_std.fillna(1e-8)\n",
    "\n",
    "        z_scores = np.abs(data - roll_mean) / roll_std\n",
    "        \n",
    "        anomaly_scores = z_scores.to_numpy()  \n",
    "        return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af35510c-9bf9-4475-91a4-27782be06200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTWModel:\n",
    "    def __init__(self, window=30, n_references=10, seed=42):\n",
    "        self.window = window\n",
    "        self.n_references = n_references\n",
    "        self.references = []\n",
    "        self.seed = seed\n",
    "    \n",
    "    def fit(self, train_sequences):\n",
    "        np.random.seed(self.seed)\n",
    "        total_sequences = train_sequences.shape[0]\n",
    "        selected_indices = np.random.choice(total_sequences, self.n_references, replace=False)\n",
    "        self.references = [train_sequences[i].squeeze().numpy() for i in selected_indices]\n",
    "        print(f\"DTWModel: Selected {self.n_references} reference sequences.\")\n",
    "    \n",
    "    def predict(self, test_sequences):\n",
    "        scores = []\n",
    "        for seq in test_sequences:\n",
    "            seq_np = seq.squeeze().numpy()\n",
    "            distances = [dtw.distance(seq_np, ref) for ref in self.references]\n",
    "            min_distance = min(distances)\n",
    "            scores.append(min_distance)\n",
    "        return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a4a9b2-fba1-4992-97c1-40a66256a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, model_name, model_params, sequence_length=30, device='cpu'):\n",
    "        self.model_name = model_name.lower()\n",
    "        self.model_params = model_params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "        \n",
    "        if self.model_name == 'egads':\n",
    "            self.model = EGADSModel(**self.model_params)\n",
    "        elif self.model_name == 'dtw':\n",
    "            self.model = DTWModel(**self.model_params)\n",
    "        else:\n",
    "            self.model = self._initialize_model().to(self.device)\n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        if self.model_name == 'autoencoder':\n",
    "            return Autoencoder(**self.model_params)\n",
    "        elif self.model_name == 'lstm':\n",
    "            return LSTMModel(**self.model_params)\n",
    "        elif self.model_name == 'cnn':\n",
    "            return CNNModel(**self.model_params)\n",
    "        elif self.model_name == 'stackvaeg':\n",
    "            return StackVAEG(**self.model_params)\n",
    "        elif self.model_name == 'omnianomaly':\n",
    "            return OmniAnomaly(**self.model_params)\n",
    "        elif self.model_name == 'cnnautoencoder': \n",
    "            return CnnAutoencoder(**self.model_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name: {self.model_name}\")\n",
    "    \n",
    "    def train(self, train_sequences=None, num_epochs=10, batch_size=32, learning_rate=1e-3, train_raw=None):\n",
    "        if self.model_name == 'egads':\n",
    "            if train_raw is None:\n",
    "                raise ValueError(\"EGADS requires 'train_raw'\")\n",
    "            data_np = train_raw.cpu().numpy().flatten()\n",
    "            self.model.fit(data_np)\n",
    "            return\n",
    "        elif self.model_name == 'dtw':\n",
    "            if train_sequences is None:\n",
    "                raise ValueError(\"DTW requires 'train_sequences'\")\n",
    "            self.model.fit(train_sequences)\n",
    "            return\n",
    "        \n",
    "        if train_sequences is None:\n",
    "            raise ValueError(f\"{self.model_name} requires 'train_sequences'\")\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_sequences, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in tqdm(range(num_epochs), desc=f\"Training {self.model_name}\"):\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if self.model_name == 'cnnautoencoder':\n",
    "                    if batch.ndimension() == 2:\n",
    "                        batch = batch.unsqueeze(1)  \n",
    "                    elif batch.ndimension() == 3:\n",
    "                        if batch.size(2) == 1:\n",
    "                            batch = batch.permute(0, 2, 1)\n",
    "                        else:\n",
    "                            if batch.size(1) != 1:\n",
    "                                raise ValueError(f\"Expected channels=1, got {batch.size(1)}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected batch shape for 'cnnautoencoder': {batch.shape}\")\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                if self.model_name in ['stackvaeg', 'omnianomaly']:\n",
    "                    if self.model_name == 'stackvaeg':\n",
    "                        outputs, mu, logvar = self.model(batch)\n",
    "                        batch_flat = batch.view(batch.size(0), -1)\n",
    "                        recon_loss = nn.MSELoss()(outputs, batch_flat)\n",
    "                    else:  \n",
    "                        reconstructed, mu, logvar = self.model(batch)\n",
    "                        recon_loss = nn.MSELoss()(reconstructed, batch)\n",
    "                    # KLD\n",
    "                    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                    loss = recon_loss + kld * 0.001\n",
    "                else:\n",
    "                    outputs = self.model(batch)\n",
    "                    if self.model_name == 'cnn':\n",
    "                        loss = nn.MSELoss()(outputs, batch.view_as(outputs))\n",
    "                    elif self.model_name == 'lstm':\n",
    "                        loss = nn.MSELoss()(outputs, batch)\n",
    "                    elif self.model_name == 'cnnautoencoder': \n",
    "                        loss = nn.MSELoss()(outputs, batch)\n",
    "                    else:\n",
    "                        loss = nn.MSELoss()(outputs, batch.view(outputs.size()))\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    def predict(self, test_sequences=None, test_raw=None):\n",
    "        if self.model_name == 'egads':\n",
    "            if test_raw is None:\n",
    "                raise ValueError(\"EGADS requires 'test_raw'\")\n",
    "            scores = self.model.predict(test_raw.cpu().numpy().flatten())\n",
    "            return scores\n",
    "        elif self.model_name == 'dtw':\n",
    "            if test_sequences is None:\n",
    "                raise ValueError(\"DTW requires 'test_sequences'\")\n",
    "            scores = self.model.predict(test_sequences)\n",
    "            return scores\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_errors = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(test_sequences)):\n",
    "                x = test_sequences[i].unsqueeze(0).to(self.device)\n",
    "\n",
    "                if self.model_name == 'cnnautoencoder':\n",
    "                # Check current dimensions\n",
    "                    if x.ndimension() == 2:\n",
    "                        # Shape: [1, sequence_length]\n",
    "                        x = x.unsqueeze(1)  # [1, 1, sequence_length]\n",
    "                    elif x.ndimension() == 3:\n",
    "                        # Shape: [batch_size, channels, sequence_length] or [batch_size, sequence_length, channels]\n",
    "                        # Assuming [1, sequence_length, channels], permute to [1, channels, sequence_length]\n",
    "                        if x.size(2) == 1:\n",
    "                            x = x.squeeze(2)  # [1, sequence_length]\n",
    "                            x = x.unsqueeze(1)  # [1, 1, sequence_length]\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unexpected input shape for 'cnnautoencoder': {x.shape}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected input shape for 'cnnautoencoder': {x.shape}\")\n",
    "                if self.model_name in ['stackvaeg', 'omnianomaly']:\n",
    "                    if self.model_name == 'stackvaeg':\n",
    "                        outputs, _, _ = self.model(x)\n",
    "                        target = x.view(1, -1)\n",
    "                        mse = torch.mean((outputs - target) ** 2, dim=1)\n",
    "                    else:\n",
    "                        reconstructed, _, _ = self.model(x)\n",
    "                        mse = torch.mean((reconstructed - x) ** 2, dim=[1,2])\n",
    "                else:\n",
    "                    outputs = self.model(x)\n",
    "                    if self.model_name == 'cnn':\n",
    "                        mse = torch.mean((outputs - x.view_as(outputs)) ** 2, dim=[1,2])\n",
    "                    elif self.model_name == 'lstm':\n",
    "                        mse = torch.mean((outputs - x) ** 2, dim=[1,2])\n",
    "                    elif self.model_name == 'cnnautoencoder': \n",
    "                        mse = torch.mean((outputs - x) ** 2, dim=[1,2])\n",
    "                    else:\n",
    "                        # autoencoder\n",
    "                        mse = torch.mean((outputs - x.view(1, -1)) ** 2, dim=1)\n",
    "                all_errors.append(mse.item())\n",
    "        return np.array(all_errors)\n",
    "    \n",
    "    def get_anomalies(self, errors, percentile=99.5):\n",
    "        threshold = np.percentile(errors, percentile)\n",
    "        anomalies = errors > threshold\n",
    "        return anomalies, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde782c3-8918-4645-a46b-72df5a422d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(mse, num_std):\n",
    "    \"\"\"Detect anomalies in test data using thresholding.\"\"\"\n",
    "    threshold = np.mean(mse) + num_std*np.std(mse)\n",
    "    #print(\"Threshold = \", np.round(threshold, 2))\n",
    "    anomalies = np.where(mse > np.round(threshold, 2))[0]\n",
    "    return anomalies\n",
    "\n",
    "def deviation_from_mean(data, value):\n",
    "    z_scores = zscore(data)\n",
    "    \n",
    "    deviation = (value - np.mean(data)) / np.std(data)\n",
    "    return deviation\n",
    "\n",
    "def simulate_trading_std(\n",
    "    model_name,\n",
    "    price_data,\n",
    "    ma_data,\n",
    "    scores,\n",
    "    rolling_anomalies,\n",
    "    num_std,\n",
    "    max_entries,\n",
    "    distr_len=99,\n",
    "):\n",
    "\n",
    "    price_array = price_data.to_numpy(dtype=np.float64)\n",
    "    ma_array = ma_data.to_numpy(dtype=np.float64)\n",
    "    anomalies_array = np.asarray(rolling_anomalies, dtype=bool)\n",
    "\n",
    "    index_array = price_data.index.to_numpy() \n",
    "\n",
    "    n = len(price_array)\n",
    "    if len(ma_array) != n or len(anomalies_array) != n:\n",
    "        raise ValueError(\"price_data, ma_data, and rolling_anomalies must have the same length.\")\n",
    "\n",
    "    initial_capital = 1.0\n",
    "    capital = initial_capital\n",
    "    capital_history_array = np.full(n, np.nan, dtype=np.float64)\n",
    "    capital_history_array[0] = capital\n",
    "\n",
    "    open_trades = []\n",
    "    trade_entries = set()  \n",
    "\n",
    "    base_price = None\n",
    "    k = 0\n",
    "    base_trade_type = None\n",
    "\n",
    "    for i in range(n):\n",
    "        timestamp = index_array[i] \n",
    "        current_price = price_array[i]\n",
    "        current_ma = ma_array[i]\n",
    "\n",
    "        if open_trades:\n",
    "            should_close_all = False\n",
    "            for trade in open_trades:\n",
    "                trade_type = trade[\"type\"]\n",
    "                entry_price = trade[\"entry_price\"]\n",
    "\n",
    "                if (trade_type == \"buy\"  and current_price >= current_ma) or \\\n",
    "                   (trade_type == \"sell\" and current_price <= current_ma):\n",
    "                    should_close_all = True\n",
    "                    break\n",
    "\n",
    "            if should_close_all:\n",
    "                for trade in open_trades:\n",
    "                    trade_type = trade[\"type\"]\n",
    "                    entry_price = trade[\"entry_price\"]\n",
    "                    exit_price = current_price\n",
    "                    trade_volume = trade[\"volume\"]\n",
    "\n",
    "                    if trade_type == \"buy\":\n",
    "                        profit = (exit_price - entry_price) * (trade_volume / entry_price)\n",
    "                    else:  \n",
    "                        profit = (entry_price - exit_price) * (trade_volume / entry_price)\n",
    "\n",
    "                    capital += profit\n",
    "                    capital_history_array[i] = capital\n",
    "\n",
    "                    print(\n",
    "                        f\"Trade executed: {trade_type} at {trade['entry_time']} \"\n",
    "                        f\"(Entry Price: {entry_price:.2f}), exited at {timestamp} \"\n",
    "                        f\"(Exit Price: {exit_price:.2f}), Volume: {trade_volume:.4f}, \"\n",
    "                        f\"P&L: {profit:.4f}\"\n",
    "                    )\n",
    "\n",
    "                open_trades.clear()\n",
    "                trade_entries.clear()\n",
    "                base_price = None\n",
    "                k = 0\n",
    "                base_trade_type = None\n",
    "\n",
    "        if anomalies_array[i] and len(open_trades) < max_entries:\n",
    "            if k == 0:\n",
    "                if timestamp not in trade_entries:\n",
    "                    trade_entries.add(timestamp)\n",
    "\n",
    "                    trade_type = None\n",
    "                    if current_price > current_ma:\n",
    "                        trade_type = \"sell\"\n",
    "                    elif current_price < current_ma:\n",
    "                        trade_type = \"buy\"\n",
    "\n",
    "                    if trade_type is not None:\n",
    "                        trade_volume = capital / max_entries\n",
    "                        open_trades.append({\n",
    "                            \"type\": trade_type,\n",
    "                            \"volume\": trade_volume,\n",
    "                            \"entry_price\": current_price,\n",
    "                            \"entry_time\": timestamp\n",
    "                        })\n",
    "                        base_price = current_price\n",
    "                        base_trade_type = trade_type\n",
    "                        k = 1\n",
    "            else:\n",
    "                start_std = max(0, i - (distr_len - 1))\n",
    "                local_slice = price_array[start_std : i+1]\n",
    "                local_std = np.std(local_slice)\n",
    "                required_distance = k * num_std * local_std\n",
    "\n",
    "                if base_trade_type == \"sell\":\n",
    "                    condition = (current_price >= base_price + required_distance)\n",
    "                elif base_trade_type == \"buy\":\n",
    "                    condition = (current_price <= base_price - required_distance)\n",
    "                else:\n",
    "                    condition = False\n",
    "\n",
    "                if condition:\n",
    "                    if timestamp not in trade_entries:\n",
    "                        trade_entries.add(timestamp)\n",
    "                        trade_type = base_trade_type\n",
    "                        trade_volume = capital / max_entries\n",
    "                        open_trades.append({\n",
    "                            \"type\": trade_type,\n",
    "                            \"volume\": trade_volume,\n",
    "                            \"entry_price\": current_price,\n",
    "                            \"entry_time\": timestamp\n",
    "                        })\n",
    "                        k += 1\n",
    "\n",
    "        capital_history_array[i] = capital\n",
    "\n",
    "    capital_history = pd.Series(capital_history_array, index=price_data.index)\n",
    "    capital_history.ffill(inplace=True)\n",
    "    capital_history.fillna(capital, inplace=True)\n",
    "\n",
    "    return capital_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9971c9d5-8933-494e-9432-cafc3655d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading_percentile(\n",
    "    model_name,\n",
    "    price_data,\n",
    "    ma_data,\n",
    "    scores,\n",
    "    rolling_percentiles, \n",
    "    rolling_anomalies,    \n",
    "    percentile,\n",
    "    max_entries,\n",
    "):\n",
    "\n",
    "    initial_capital = 1.0\n",
    "    capital = initial_capital\n",
    "    capital_history = pd.Series(index=price_data.index, data=np.nan)\n",
    "    capital_history.iloc[0] = capital\n",
    "\n",
    "    open_trades = []\n",
    "    trade_entries = set()  # Timestamps where trades are opened\n",
    "\n",
    "    index_array = list(price_data.index) \n",
    "\n",
    "    for i in range(len(index_array)):\n",
    "        timestamp = index_array[i]\n",
    "\n",
    "        current_price = price_data.iloc[i]\n",
    "        current_ma    = ma_data.iloc[i]\n",
    "        trades_to_close = []\n",
    "        for trade in open_trades:\n",
    "            trade_type = trade[\"type\"]\n",
    "            entry_price = trade[\"entry_price\"]\n",
    "            entry_time = trade[\"entry_time\"] \n",
    "            anomaly_percentage = trade[\"anomaly_percentage\"]  \n",
    "            if trade_type == \"buy\" and current_price >= current_ma:\n",
    "                exit_price = current_price\n",
    "                profit = (exit_price - entry_price) * (trade[\"volume\"] / entry_price)\n",
    "                capital += profit\n",
    "                trades_to_close.append(trade)\n",
    "                capital_history.iloc[i] = capital\n",
    "\n",
    "                print(\n",
    "                    f\"Trade executed: {trade_type} at {entry_time} \"\n",
    "                    f\"(Entry Price: {entry_price:.2f}), exited at {timestamp} \"\n",
    "                    f\"(Exit Price: {exit_price:.2f}), Volume: {trade['volume']:.4f}, \"\n",
    "                    f\"Anomaly: {anomaly_percentage:.2f}%, P&L: {profit:.4f}\"\n",
    "                )\n",
    "\n",
    "            elif trade_type == \"sell\" and current_price <= current_ma:\n",
    "                exit_price = current_price\n",
    "                profit = (entry_price - exit_price) * (trade[\"volume\"] / entry_price)\n",
    "                capital += profit\n",
    "                trades_to_close.append(trade)\n",
    "                capital_history.iloc[i] = capital\n",
    "\n",
    "                print(\n",
    "                    f\"Trade executed: {trade_type} at {entry_time} \"\n",
    "                    f\"(Entry Price: {entry_price:.2f}), exited at {timestamp} \"\n",
    "                    f\"(Exit Price: {exit_price:.2f}), Volume: {trade['volume']:.4f}, \"\n",
    "                    f\"Anomaly: {anomaly_percentage:.2f}%, P&L: {profit:.4f}\"\n",
    "                )\n",
    "\n",
    "        for closed_trade in trades_to_close:\n",
    "            open_trades.remove(closed_trade)\n",
    "\n",
    "        if rolling_anomalies[i] and len(open_trades) < max_entries:\n",
    "            k = len(open_trades)  \n",
    "            required_threshold = percentile + k * step \n",
    "            if required_threshold >= 100.0:\n",
    "                required_threshold = 100.1\n",
    "\n",
    "            anomaly_percentile = rolling_percentiles[i]  \n",
    "            if anomaly_percentile >= required_threshold: \n",
    "                if timestamp not in trade_entries:\n",
    "                    trade_entries.add(timestamp)\n",
    "                    if current_price > current_ma:\n",
    "                        trade_type = \"sell\"\n",
    "                    elif current_price < current_ma:\n",
    "                        trade_type = \"buy\"\n",
    "                    else:\n",
    "                        trade_type = None\n",
    "\n",
    "                    if trade_type is not None:\n",
    "                        trade_volume = capital / max_entries\n",
    "                        anomaly_percentage = 100.0 - anomaly_percentile  \n",
    "                        open_trades.append({\n",
    "                            \"type\": trade_type,\n",
    "                            \"volume\": trade_volume,\n",
    "                            \"entry_price\": current_price,\n",
    "                            \"entry_time\": timestamp,\n",
    "                            \"anomaly_percentage\": anomaly_percentage  \n",
    "                        })\n",
    "\n",
    "\n",
    "        capital_history.iloc[i] = capital\n",
    "\n",
    "    capital_history.ffill(inplace=True)\n",
    "    capital_history.fillna(capital, inplace=True)\n",
    "\n",
    "    return capital_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a34f468-e411-47be-8ef3-4e4733ca190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window_days = 55\n",
    "window_minutes = 500 #for rolling mean\n",
    "percentile = 99.5\n",
    "max_entries = 5\n",
    "step = (100 - percentile) / max_entries\n",
    "\n",
    "mode = 'std' #'std'/'percentile'\n",
    "num_std = 1\n",
    "\n",
    "window_size_minutes = 1000 #window for final anomalies distribution\n",
    "\n",
    "original_data_sorted = data[['open', 'close']].copy().sort_index()\n",
    "\n",
    "original_data_sorted['rolling_avg'] = original_data_sorted['open'].rolling(window=window_minutes).mean()\n",
    "original_data_sorted['rolling_low'] = original_data_sorted['open'].rolling(window=window_minutes).min()\n",
    "original_data_sorted['rolling_high'] = original_data_sorted['open'].rolling(window=window_minutes).max()\n",
    "\n",
    "\n",
    "def analyze_anomalies(anomaly_timestamps, original_data_sorted, window_minutes=13*24*60):\n",
    "    results = []\n",
    "    \n",
    "    for ts in anomaly_timestamps:\n",
    "        if ts < original_data_sorted.index[window_minutes-1]:\n",
    "            continue\n",
    "        \n",
    "        future_time = ts + pd.Timedelta(hours=1)\n",
    "        if future_time > original_data_sorted.index[-1]:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            open_price = original_data_sorted.loc[ts, 'open']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        prev_time = ts - pd.Timedelta(minutes=1)\n",
    "        if prev_time not in original_data_sorted.index:\n",
    "            continue\n",
    "        rolling_avg = original_data_sorted.loc[prev_time, 'rolling_avg']\n",
    "        rolling_low = original_data_sorted.loc[prev_time, 'rolling_low']\n",
    "        rolling_high = original_data_sorted.loc[prev_time, 'rolling_high']\n",
    "        \n",
    "        if np.isnan(rolling_avg) or np.isnan(rolling_low) or np.isnan(rolling_high):\n",
    "            continue\n",
    "        \n",
    "        above_avg = open_price > rolling_avg\n",
    "        \n",
    "        above_low = open_price > rolling_low\n",
    "        below_high = open_price < rolling_high\n",
    "        \n",
    "        try:\n",
    "            next_hour_data = original_data_sorted.loc[ts + pd.Timedelta(minutes=1): ts + pd.Timedelta(hours=1)]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        if next_hour_data.empty:\n",
    "            continue\n",
    "        \n",
    "        min_open_next_hour = next_hour_data['open'].min()\n",
    "        max_open_next_hour = next_hour_data['open'].max()\n",
    "        \n",
    "        try:\n",
    "            close_price_next_hour = original_data_sorted.loc[ts + pd.Timedelta(hours=1), 'close']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        if above_avg:\n",
    "            price_after = min_open_next_hour\n",
    "            price_change = min_open_next_hour - open_price\n",
    "            direction = 'Min'\n",
    "        else:\n",
    "            price_after = max_open_next_hour\n",
    "            price_change = max_open_next_hour - open_price\n",
    "            direction = 'Max'\n",
    "        \n",
    "        close_change = close_price_next_hour - open_price\n",
    "        \n",
    "        price_change_pct = (price_change / open_price) * 100\n",
    "        close_change_pct = (close_change / open_price) * 100\n",
    "        \n",
    "        if above_avg:\n",
    "            if price_after < open_price:\n",
    "                sign = 1  \n",
    "            elif price_after > open_price:\n",
    "                sign = -1  \n",
    "            else:\n",
    "                sign = 0\n",
    "        else:\n",
    "            if price_after > open_price:\n",
    "                sign = 1  \n",
    "            elif price_after < open_price:\n",
    "                sign = -1  \n",
    "            else:\n",
    "                sign = 0\n",
    "        \n",
    "        price_change_pct_signed = price_change_pct * sign\n",
    "        close_change_pct_signed = close_change_pct * sign\n",
    "        \n",
    "        relative_to_avg = 'Above Avg' if above_avg else 'Below Avg'\n",
    "        relative_to_low_high = []\n",
    "        if above_low:\n",
    "            relative_to_low_high.append('Above Low')\n",
    "        if below_high:\n",
    "            relative_to_low_high.append('Below High')\n",
    "        if not relative_to_low_high:\n",
    "            relative_to_low_high.append('At Low/High')  \n",
    "        \n",
    "        results.append({\n",
    "            'timestamp': ts,\n",
    "            'open_price': open_price,\n",
    "            'rolling_avg': rolling_avg,\n",
    "            'rolling_low': rolling_low,\n",
    "            'rolling_high': rolling_high,\n",
    "            'relative_to_avg': relative_to_avg,\n",
    "            'relative_to_low_high': ', '.join(relative_to_low_high),\n",
    "            'price_after': price_after,\n",
    "            'price_change_pct_signed': price_change_pct_signed,\n",
    "            'direction': direction,\n",
    "            'close_next_hour': close_price_next_hour,\n",
    "            'close_change_pct_signed': close_change_pct_signed\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def compute_statistics(results_df, model_name):\n",
    "    if results_df.empty:\n",
    "        print(\"No anomalies to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Statistics for {model_name.capitalize()} ===\")\n",
    "    \n",
    "    relative_to_avg = results_df['relative_to_avg']\n",
    "    print(f\"\\nRelative to {window_minutes}-minutes Average:\")\n",
    "    print(relative_to_avg.value_counts())\n",
    "    \n",
    "    relative_to_low_high = results_df['relative_to_low_high']\n",
    "    print(f\"\\nRelative to {window_minutes}-minutes Low/High:\")\n",
    "    print(relative_to_low_high.value_counts())\n",
    "    \n",
    "    print(\"\\nNext Hour Min/Max Open Price Change (%):\")\n",
    "    print(results_df['price_change_pct_signed'].describe())\n",
    "    \n",
    "    print(\"\\nNext Hour Close Price Change (%):\")\n",
    "    print(results_df['close_change_pct_signed'].describe())\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    relative_to_avg.value_counts().plot(kind='bar')\n",
    "    plt.title(f'{model_name.capitalize()} - Relative to {window_minutes}-minutes Average')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    relative_to_low_high.value_counts().plot(kind='bar')\n",
    "    plt.title(f'{model_name.capitalize()} - Relative to {window_minutes}-minutes Low/High')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    results_df['price_change_pct_signed'].hist(bins=30)\n",
    "    plt.title(f'{model_name.capitalize()} - Next Hour Min/Max Open Price Change (%) Distribution')\n",
    "    plt.xlabel('Price Change (%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    results_df['close_change_pct_signed'].hist(bins=30)\n",
    "    plt.title(f'{model_name.capitalize()} - Next Hour Close Price Change (%) Distribution')\n",
    "    plt.xlabel('Price Change (%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b69959-2007-4b23-a778-5c9db21e7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = ['autoencoder', 'cnnautoencoder', 'lstm', 'cnn', 'stackvaeg', 'omnianomaly', 'egads'] ######\n",
    "model_names = ['cnnautoencoder']\n",
    "model_params = {\n",
    "    'autoencoder': {\n",
    "        'input_size': sequence_length * 1,  \n",
    "        'hidden_dims': [64, 32, 16]\n",
    "    },\n",
    "    'lstm': {\n",
    "        'input_size': 1,     \n",
    "        'hidden_size': 32,\n",
    "        'num_layers': 1\n",
    "    },\n",
    "    'cnn': {\n",
    "        'input_size': sequence_length,\n",
    "        'out_channels': 16,\n",
    "        'kernel_size': 3\n",
    "    },\n",
    "    'stackvaeg': {\n",
    "        'input_size': sequence_length * 1,  \n",
    "        'hidden_dims': [64, 32],\n",
    "        'latent_dim': 16\n",
    "    },\n",
    "    'omnianomaly': {\n",
    "        'input_size': 1,     \n",
    "        'hidden_size': 32,\n",
    "        'latent_dim': 16,\n",
    "        'num_layers': 1\n",
    "    },\n",
    "    'egads': {\n",
    "        'window': 30\n",
    "    },\n",
    "    'dtw': {\n",
    "        'window': 30,\n",
    "        'n_references': 10,\n",
    "        'seed': 42\n",
    "    },\n",
    "    'cnnautoencoder': {  \n",
    "        'input_channels': 1,\n",
    "        'sequence_length': sequence_length,\n",
    "        'hidden_dims': [32, 16] \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a83162-b685-43a8-a949-1cdd42715750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_initial_length = pd.DateOffset(months=6)  \n",
    "train_expand_step = pd.DateOffset(months=3)    \n",
    "test_length = pd.DateOffset(months=3)           \n",
    "\n",
    "start_date = pd.Timestamp('2020-01-01 00:00:00')\n",
    "end_date = pd.Timestamp('2021-12-31 23:59:00')\n",
    "\n",
    "combined_test_dates = []      \n",
    "combined_scores = {}          \n",
    "combined_anomalies = {}  \n",
    "original_prices_buffer = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    combined_scores[model_name] = []\n",
    "    combined_anomalies[model_name] = []\n",
    "\n",
    "current_train_start = start_date\n",
    "current_train_end = current_train_start + train_initial_length\n",
    "current_test_start = current_train_end\n",
    "current_test_end = current_test_start + test_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e756b-6df1-4ecd-af8f-c66c198adce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window: Train 2020-01-01 00:00:00 to 2020-07-01 00:00:00, Test 2020-07-01 00:00:00 to 2020-10-01 00:00:00 ===\n",
      "\n",
      "=== Processing Model: cnnautoencoder ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  10%|██████████████▎                                                                                                                                | 1/10 [01:04<09:38, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.002423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  20%|████████████████████████████▌                                                                                                                  | 2/10 [02:06<08:23, 62.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.000060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  30%|██████████████████████████████████████████▉                                                                                                    | 3/10 [03:34<08:42, 74.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  40%|█████████████████████████████████████████████████████████▏                                                                                     | 4/10 [05:16<08:30, 85.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.000037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 5/10 [06:31<06:48, 81.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 6/10 [07:42<05:12, 78.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.000029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 7/10 [08:52<03:46, 75.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 8/10 [09:52<02:20, 70.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 9/10 [10:55<01:08, 68.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.000022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [11:55<00:00, 71.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window: Train 2020-01-01 00:00:00 to 2020-10-01 00:00:00, Test 2020-10-01 00:00:00 to 2021-01-01 00:00:00 ===\n",
      "\n",
      "=== Processing Model: cnnautoencoder ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  10%|██████████████▏                                                                                                                               | 1/10 [01:58<17:43, 118.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.002531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  20%|████████████████████████████▍                                                                                                                 | 2/10 [04:16<17:18, 129.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  30%|██████████████████████████████████████████▌                                                                                                   | 3/10 [06:05<14:02, 120.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  40%|████████████████████████████████████████████████████████▊                                                                                     | 4/10 [08:13<12:19, 123.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  50%|███████████████████████████████████████████████████████████████████████                                                                       | 5/10 [10:32<10:45, 129.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  60%|█████████████████████████████████████████████████████████████████████████████████████▏                                                        | 6/10 [12:35<08:28, 127.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 7/10 [14:10<05:49, 116.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 8/10 [15:57<03:46, 113.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 9/10 [18:07<01:58, 118.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [20:19<00:00, 121.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window: Train 2020-01-01 00:00:00 to 2021-01-01 00:00:00, Test 2021-01-01 00:00:00 to 2021-04-01 00:00:00 ===\n",
      "\n",
      "=== Processing Model: cnnautoencoder ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  10%|██████████████▏                                                                                                                               | 1/10 [02:41<24:09, 161.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.000959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  20%|████████████████████████████▍                                                                                                                 | 2/10 [05:20<21:21, 160.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.000036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  30%|██████████████████████████████████████████▌                                                                                                   | 3/10 [08:03<18:50, 161.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  40%|████████████████████████████████████████████████████████▊                                                                                     | 4/10 [10:44<16:08, 161.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  50%|███████████████████████████████████████████████████████████████████████                                                                       | 5/10 [13:30<13:35, 163.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  60%|█████████████████████████████████████████████████████████████████████████████████████▏                                                        | 6/10 [16:13<10:51, 162.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 7/10 [18:53<08:05, 161.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 8/10 [21:43<05:28, 164.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 9/10 [24:26<02:44, 164.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [26:57<00:00, 161.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window: Train 2020-01-01 00:00:00 to 2021-04-01 00:00:00, Test 2021-04-01 00:00:00 to 2021-07-01 00:00:00 ===\n",
      "\n",
      "=== Processing Model: cnnautoencoder ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  10%|██████████████▏                                                                                                                               | 1/10 [03:59<35:52, 239.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.000973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  20%|████████████████████████████▍                                                                                                                 | 2/10 [07:13<28:22, 212.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.000022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  30%|██████████████████████████████████████████▌                                                                                                   | 3/10 [10:29<23:56, 205.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  40%|████████████████████████████████████████████████████████▊                                                                                     | 4/10 [13:40<19:56, 199.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  50%|███████████████████████████████████████████████████████████████████████                                                                       | 5/10 [16:49<16:18, 195.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  60%|█████████████████████████████████████████████████████████████████████████████████████▏                                                        | 6/10 [20:10<13:09, 197.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 7/10 [23:00<09:25, 188.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 8/10 [25:58<06:10, 185.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cnnautoencoder:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 9/10 [29:06<03:06, 186.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.000008\n"
     ]
    }
   ],
   "source": [
    "while current_test_end <= end_date or current_test_start < end_date:\n",
    "    if current_test_end > end_date:\n",
    "        current_test_end = end_date\n",
    "    print(f\"\\n=== Processing Window: Train {current_train_start} to {current_train_end}, \"\n",
    "          f\"Test {current_test_start} to {current_test_end} ===\")\n",
    "    \n",
    "    train_data = data.loc[current_train_start:current_train_end]\n",
    "    test_data = data.loc[current_test_start:current_test_end]\n",
    "\n",
    "    if test_data.empty:\n",
    "        break\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data[['open']])\n",
    "    test_scaled = scaler.transform(test_data[['open']])\n",
    "\n",
    "    original_prices_buffer.extend(test_data['open'].values[sequence_length:])\n",
    "    \n",
    "    train_torch = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "    test_torch = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    train_sequences = create_sequences(train_torch, sequence_length)\n",
    "    test_sequences = create_sequences(test_torch, sequence_length)\n",
    "    test_dates = test_data.index[sequence_length:]\n",
    "    combined_test_dates.extend(test_dates)\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\n=== Processing Model: {model_name} ===\")\n",
    "        \n",
    "        params = model_params[model_name]\n",
    "        detector = AnomalyDetector(\n",
    "            model_name=model_name,\n",
    "            model_params=params,\n",
    "            sequence_length=sequence_length,\n",
    "            device='cpu' \n",
    "        )\n",
    "        \n",
    "        if model_name not in ['egads', 'dtw']:\n",
    "            detector.train(\n",
    "                train_sequences=train_sequences,\n",
    "                num_epochs=10,\n",
    "                batch_size=32,\n",
    "                learning_rate=1e-3\n",
    "            )\n",
    "        elif model_name == 'egads':\n",
    "            detector.train(\n",
    "                train_sequences=None,\n",
    "                train_raw=train_torch\n",
    "            )\n",
    "        elif model_name == 'dtw':\n",
    "            detector.train(\n",
    "                train_sequences=train_sequences,\n",
    "                num_epochs=0,\n",
    "                batch_size=0,\n",
    "                learning_rate=0,\n",
    "                train_raw=None\n",
    "            )\n",
    "        \n",
    "        if model_name not in ['egads', 'dtw']:\n",
    "            scores = detector.predict(test_sequences=test_sequences)\n",
    "        elif model_name == 'egads':\n",
    "            scores = detector.predict(test_raw=test_torch)[sequence_length:]\n",
    "        elif model_name == 'dtw':\n",
    "            scores = detector.predict(test_sequences=test_sequences)\n",
    "        \n",
    "        combined_scores[model_name].extend(scores)\n",
    "    \n",
    "    current_train_end += train_expand_step\n",
    "    current_test_start += train_expand_step\n",
    "    current_test_end = current_test_start + test_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad52b1a-8048-4cfb-a932-0918ad61d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for model_name, scores in combined_scores.items():\n",
    "    scores_array = np.array(scores)\n",
    "    \n",
    "    np.save(f\"{model_name}_combined_scores_3m.npy\", scores_array)\n",
    "    print(f\"Saved combined_scores for {model_name} to {model_name}_combined_scores.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bf3c2f7-617d-4efa-b6c6-5304b833c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scores['autoencoder'] = np.load(\"autoencoder_combined_scores_3m.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41023a4a-7f79-41bc-847c-00dcc6fb2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"\\n=== Final Outputs for Model: {model_name} ===\")\n",
    "\n",
    "    combined_scores[model_name] = np.array(combined_scores[model_name])\n",
    "    scores = np.array(combined_scores[model_name])\n",
    "\n",
    "    rolling_percentiles = np.zeros_like(scores, dtype=float) \n",
    "    rolling_anomalies   = np.zeros_like(scores, dtype=bool)  \n",
    "    rolling_stds = np.zeros_like(scores, dtype=float) \n",
    "\n",
    "    for i in range(window_size_minutes, len(scores)):\n",
    "        start_idx = i - window_size_minutes\n",
    "        end_idx = i  \n",
    "        window_slice = scores[start_idx:end_idx]  \n",
    "\n",
    "        current_score = scores[i]\n",
    "\n",
    "        if mode == 'percentile' or 'std':\n",
    "            rank = percentileofscore(window_slice, current_score, kind='rank')\n",
    "            rolling_percentiles[i] = rank  \n",
    "    \n",
    "            if rank >= percentile:\n",
    "                rolling_anomalies[i] = True\n",
    "        elif mode == 'stdd':\n",
    "            deviation = deviation_from_mean(window_slice[-99:], current_score)\n",
    "            rolling_stds[i] = deviation  \n",
    "            \n",
    "            if deviation >= num_std:\n",
    "                rolling_anomalies[i] = True\n",
    "        else:\n",
    "            raise ValueError(\"Mode should be 'std' of 'percentile'\")\n",
    "            \n",
    "\n",
    "    anomaly_dates = np.array(combined_test_dates)[rolling_anomalies]\n",
    "    print(f\"Found {rolling_anomalies.sum()} anomalies using rolling percentile = {percentile}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    unscaled_prices = np.array(original_prices_buffer)\n",
    "    plt.plot(combined_test_dates, unscaled_prices, label='Open Price', color='blue')\n",
    "\n",
    "    rolling_avg_values = original_data_sorted['rolling_avg'].reindex(combined_test_dates).values\n",
    "    plt.plot(combined_test_dates, rolling_avg_values, label='Rolling Avg', color='orange', linewidth=2)\n",
    "\n",
    "    anomaly_prices = unscaled_prices[rolling_anomalies]\n",
    "    plt.scatter(anomaly_dates, anomaly_prices, color='red', label='Anomalies')\n",
    "    plt.title(f'{model_name.capitalize()} Anomaly Detection (Rolling) for Entire Test Period')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Open Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    analysis_results = analyze_anomalies(anomaly_dates, original_data_sorted, window_minutes=window_minutes)\n",
    "    compute_statistics(analysis_results, model_name)\n",
    "\n",
    "    ma_window = window_minutes\n",
    "    original_data_sorted['MA'] = original_data_sorted['open'].rolling(window=ma_window, min_periods=1).mean()\n",
    "    \n",
    "    combined_test_data = original_data_sorted.loc[combined_test_dates, 'open']\n",
    "    combined_ma_data = original_data_sorted.loc[combined_test_dates, 'MA']\n",
    "\n",
    "    if mode == 'percentile':\n",
    "        capital_history = simulate_trading_percentile(\n",
    "            model_name=model_name,\n",
    "            price_data=original_data_sorted.loc[combined_test_dates, 'open'],\n",
    "            ma_data=original_data_sorted.loc[combined_test_dates, 'MA'],\n",
    "            scores=scores,\n",
    "            rolling_percentiles=rolling_percentiles,  \n",
    "            rolling_anomalies=rolling_anomalies,      \n",
    "            percentile=percentile,\n",
    "            max_entries=max_entries,\n",
    "        )\n",
    "    elif mode == 'std':\n",
    "        capital_history = simulate_trading_std(\n",
    "            model_name=model_name,\n",
    "            price_data=original_data_sorted.loc[combined_test_dates, 'open'],\n",
    "            ma_data=original_data_sorted.loc[combined_test_dates, 'MA'],\n",
    "            scores=scores,\n",
    "            rolling_anomalies=rolling_anomalies,      \n",
    "            num_std=num_std,\n",
    "            max_entries=max_entries,\n",
    "            distr_len=99\n",
    "        )\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(capital_history.index, capital_history.values - 1, label='Capital')\n",
    "    plt.title(f'P&L Curve for {model_name.capitalize()} (Rolling Window Simulation)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    final_capital = capital_history.iloc[-1]\n",
    "    print(f\"Final Capital for {model_name}: {final_capital:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6edb0-2634-42b9-bc2a-710766053e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
